---
slug: along-duo-habitats
status: proof
title: Along Duo Habitats
event: concert-7-la-nau
type: performance
submission_type: Performance
time: Friday, May 30, 21:30 - 02:00
contributors:
- person: $hoogland-timo
- person: $bautista-lina

---

# $PROGRAM_NOTE

Along Duo Habitats is a hybrid instrumental and coding collaboration between Lina Bautista (ES)
and Timo Hoogland (NL). In the performance they combine drums, electric guitar and live coding.

They use the programming language Mercury to create electronic music with the collaborative code-
editor Flok. During the performance the computer acts as a co-performer making suggestions in
the code while listening and reacting to patterns played on the drums and guitar. Parts of the
code are pre-coded snippets while other parts are generated in real-time. By giving the computer
some agency in the performance they will have to react and improvise to the sound, resulting in
a dialogue between the analog and digital worlds of all performers. All this in an attempt to find a
balance between playing live instruments, coding live at a high level of abstraction, preparing code
in advance, and giving the computer agency in parts of the performance. 

This project has been granted funding by the Creative Industries Fund NL.

# $ABSTRACT

Along Duo Habitats is a hybrid instrumental and coding collaboration between Lina Bautista (ES)
and Timo Hoogland (NL). In the performance they combine drums, electric guitar and live coding.
They use the programming language Mercury to create electronic music with the collaborative code-
editor Flok. During the performance the computer acts as a co-performer making suggestions in
the code while listening and reacting to patterns played on the drums and guitar. Parts of the
code are pre-coded snippets while other parts are generated in real-time. By giving the computer
some agency in the performance they will have to react and improvise to the sound, resulting in
a dialogue between the analog and digital worlds of all performers. All this in an attempt to find a
balance between playing live instruments, coding live at a high(er) level of abstraction, preparing
code in advance, and giving the computer agency in parts of the performance.

During the performance the computer acts as a co-performer/semi-autonomous agent making
changes and suggestions in the code while listening and reacting to patterns and timbres played on
the instruments. A small network is trained to classify various patterns that are composed before-
hand and it will make suggestions based on the perceived sounds. The patterns are recognized via
contact microphones attached to the drums, and the direct input from the guitar. Onset detection of
the instruments, and various other sound descriptors such as pitch and timbre (MFCC), are derived
from the incoming sound using a MaxMSP patch with the help of the FluCoMa library. Patterns have
been connected to highly abstracted gestures such as remove code, add code and replace code.
This results in a “comprovisation” (partially improvised, partially composed) together with the com-
puter. Some decisions are made psuedorandomly through processes such as a markov chain. The
descriptors are also used to be able to control coded synthesizers and samplers via onset triggers,
derived pitch, playing speed, note density, and more.

In our performance we focus on bringing our instruments, that we usually use in other contexts
than live coding, to the context of live coding and attempt to combine playing them together with
live coding. We acknowledge that live coding and playing an instrument at the same time is difficult,
therefore the automated decisions by the computer will help us perform live and allow us to make
gestures at a higher level of abstraction. Parts of the code that are generated by the computer have
been pre-coded as small snippets. By switching between typing code, generating code through
playing the instruments, and letting the computer decide, we explore various ways of liveness and
improvisation in the code.

Furthermore we work in one single editor for coding music, Flok. The Mercury parser always eval-
uates the full page, meaning that the visible code always acts as a direct reflection of the current
state of the sound and the system. For example if a line is commented, that sound will be removed
from the composition. The benefit of this approach is that we as collaborators are always aware of
what instruments are sounding in the music and can easily locate them in the code. A downside
is that we have to take extra care when evaluating code if someone else is still in the middle of
typing. However this form of collaborating has increased our attentive listening skills and urged us
to rehearse more so we know the direction of the performance together.

Since the inception of live coding there have been various succesful attempts at collaborative live
coding performances. Such as the performances by T.Y.P.E. (The Yorkshire Programming Ensem-
ble) using Troop and Foxdot and the Estuary Weekendjams on the Eulerroom channel hosted by
Cleary and Joanq. Besides that there have also been a few performances combining physical in-
struments with live coding, for example Canute by Alex McLean and Yee-King where one is live
coding while the other is playing electronic drums, and the CodeKlavier by Anne Veinberg and
Felipe Ignacio Noriega, using the acoustic piano for live coding. There have also been attempts
at using neural networks to generate code or give coding suggestions during a performance. In
the Cibo project by Jeremy Stewart and Shawn Lawson a computer was autonomously generating
TidalCycles code on stage, and in another performance Elizabeth Wilson was live coding together
with an affective autonomous agent, also in TidalCycles.

We are also heavily influenced by various music groups that operate in different genres and scenes
than the live coding scene. For example we have been taking inspiration from other live electronic
duo performances such as Overmono and Moderat where we analyzed how both performers are
being complementary to eachother, taking care of different tasks. On the other hand we are inspired
by bands such as Nerve, 65dos and Aiming for Enrike, where live electronics, live instruments, and
interactions between those are composed in various interesting ways.

The Mercury programming environment is an open source live coding language developed by Timo
Hoogland (NL). The language has been designed around making code more accessible and less
obfuscating for the performer as well as the audience. Flok (https://flok.cc) is an open source
collaborative coding editor developed by Damián Silvani (AR) that runs in the browser. Multiple
users can join together in the same editor by accessing the same url.

This project has been granted funding by the Creative Industries Fund NL (Stimuleringsfonds Cre-
atieve Industrie).

